# MNIST-AI
Artificial Feedforward neural network coded in pure java (no libraries used). This is an application I've developed from the training process that visualises the performance. The network is not perfect by any means. It has an accuracy of 91.7% which is good but no great. This is because my architectrue (784,16,16,10) is imperfect and is unoptimised for training. The model used in the visualisation tool is was trained using mini-batch stochastic gradient descent. Note: due to misundersatnding, my model wasn`t trained using traditional epochs, instead each iteration just chooses random test data (of the mini-batch size). To run, unzip the zipped folder. Simply, run the jar file in that unzipped folder. Enjoy! Note the zip file "MNIST" is an earlier version of "MNSIT AI", both are very similar, "MNIST AI" has all the functionality that "MNIST" has, it just has more (namely, visulaising the ouptut of the network). AI Tool.png is a screenshot of what the application "MNIST AI" looks like when running. Update: Best AI Zip file is a new file I trained with an architecture of 784,128,128,10 using a bias and weight for each neuron. Sigmoid activation is used in all layers. Adam optimiser is employed and the model is trained using pure stochastic gradient descent. Ai Tool Best png is an image of the tool in action. It has an accuracy of 97.12%. Finally, using reLU and the adam optimiser and softmax for output layer, I was able to achieve an accuracy of 98.99% with a learning rate of 0.001, epsillon of 1*10^-8 and an architecture of 784,1024,512,512,10 (although the last accuracy increase was somewhat random as model converged at around 98.5% realistically, the rest was essentially just stumbling around the local minimum for a long period of time.
